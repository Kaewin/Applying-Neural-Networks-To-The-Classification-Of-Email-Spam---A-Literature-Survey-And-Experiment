{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import email\n",
    "import re\n",
    "# import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from email.message import EmailMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stopwords in c:\\anaconda3\\envs\\learn-env\\lib\\site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gerba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gerba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Plain Text Files To .eml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eml(root_folder, output_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # Process only files (without extension)\n",
    "            if not os.path.splitext(filename)[1]:\n",
    "                output_path = os.path.join(output_folder, f'{filename}.eml')\n",
    "\n",
    "                try:\n",
    "                    # Read raw text content with UTF-8 encoding\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        raw_message = file.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip files not in UTF-8 encoding\n",
    "                    continue\n",
    "\n",
    "                # Create an EmailMessage object\n",
    "                eml = EmailMessage()\n",
    "                eml.set_content(raw_message)\n",
    "\n",
    "                # Save as EML file\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    output_file.write(eml.as_bytes())\n",
    "\n",
    "# Usage example\n",
    "root_folder = 'data\\enron\\ham'\n",
    "output_folder = 'data\\enron_eml_ham'\n",
    "\n",
    "convert_to_eml(root_folder, output_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eml_spam(root_folder, output_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # Process only files with .txt extension\n",
    "            if os.path.splitext(filename)[1] == '.txt':\n",
    "                output_path = os.path.join(output_folder, f'{filename}.eml')\n",
    "\n",
    "                try:\n",
    "                    # Read raw text content with UTF-8 encoding\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        raw_message = file.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip files not in UTF-8 encoding\n",
    "                    continue\n",
    "\n",
    "                # Create an EmailMessage object\n",
    "                eml = EmailMessage()\n",
    "                eml.set_content(raw_message)\n",
    "\n",
    "                # Save as EML file\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    output_file.write(eml.as_bytes())\n",
    "\n",
    "# Usage example\n",
    "root_folder = 'data\\enron\\spam'\n",
    "output_folder = 'data\\enron_eml_spam'\n",
    "\n",
    "convert_to_eml_spam(root_folder, output_folder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_info(root_folder, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Number of Unique Stemmed Words', 'Number of Lemmatized Words', 'Cleaned Body', 'Target'])\n",
    "\n",
    "        index = 1\n",
    "\n",
    "        for root, dirs, files in os.walk(root_folder):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "\n",
    "                # Process only files with .eml extension\n",
    "                if os.path.splitext(filename)[1] == '.eml':\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        eml_data = file.read()\n",
    "\n",
    "                    msg = email.message_from_bytes(eml_data)\n",
    "\n",
    "                    message_body = ''\n",
    "                    if msg.is_multipart():\n",
    "                        for part in msg.walk():\n",
    "                            content_type = part.get_content_type()\n",
    "                            if content_type == 'text/plain':\n",
    "                                message_body = part.get_payload(decode=True).decode('utf-8')\n",
    "                                break\n",
    "                    else:\n",
    "                        message_body = msg.get_payload(decode=True).decode('utf-8')\n",
    "\n",
    "                    features = {\n",
    "                        'number_of_words': len(message_body.split()),\n",
    "                        'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "                        'number_of_unique_words': len(set(message_body.split())),\n",
    "                        'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "                        'number_of_exclamation_points': message_body.count('!'),\n",
    "                    }\n",
    "\n",
    "                    target = 0 # 0 for ham, 1 for spam\n",
    "\n",
    "                    # Tokenize the message body\n",
    "                    tokens = nltk.word_tokenize(message_body)\n",
    "\n",
    "                    # Remove stop words\n",
    "                    stop_words = nltk.corpus.stopwords.words('english')\n",
    "                    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "                    # Stem the tokens\n",
    "                    stemmer = nltk.stem.PorterStemmer()\n",
    "                    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "                    # Lemmatize the tokens\n",
    "                    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "                    features['number_of_unique_stemmed_words'] = len(set(stemmed_tokens))\n",
    "                    features['number_of_lemmatized_words'] = len(set(lemmatized_tokens))\n",
    "\n",
    "                    cleaned_body = ' '.join([stemmer.stem(token) for token in message_body.split() if token not in stop_words])\n",
    "\n",
    "                    writer.writerow([index, message_body] + list(features.values()) + [cleaned_body] + [target])\n",
    "\n",
    "                    index += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root_folder = 'data\\enron_eml_ham'\n",
    "    output_csv = 'data\\enron_proccessed.csv'\n",
    "\n",
    "    extract_email_info(root_folder, output_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3217\n"
     ]
    }
   ],
   "source": [
    "print(1282 + 1935)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_info(root_folder, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Number of Unique Stemmed Words', 'Number of Lemmatized Words', 'Cleaned Body', 'Target'])\n",
    "\n",
    "        index = 1\n",
    "        spam_count = 0\n",
    "\n",
    "        for root, dirs, files in os.walk(root_folder):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "\n",
    "                # Process only files with .eml extension\n",
    "                if os.path.splitext(filename)[1] == '.eml':\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        eml_data = file.read()\n",
    "\n",
    "                    msg = email.message_from_bytes(eml_data)\n",
    "\n",
    "                    message_body = ''\n",
    "                    if msg.is_multipart():\n",
    "                        for part in msg.walk():\n",
    "                            content_type = part.get_content_type()\n",
    "                            if content_type == 'text/plain':\n",
    "                                message_body = part.get_payload(decode=True).decode('utf-8')\n",
    "                                break\n",
    "                    else:\n",
    "                        message_body = msg.get_payload(decode=True).decode('utf-8')\n",
    "\n",
    "                    features = {\n",
    "                        'number_of_words': len(message_body.split()),\n",
    "                        'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "                        'number_of_unique_words': len(set(message_body.split())),\n",
    "                        'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "                        'number_of_exclamation_points': message_body.count('!'),\n",
    "                    }\n",
    "\n",
    "                    target = 1 # 0 for ham, 1 for spam\n",
    "\n",
    "                    # Tokenize the message body\n",
    "                    tokens = nltk.word_tokenize(message_body)\n",
    "\n",
    "                    # Remove stop words\n",
    "                    stop_words = nltk.corpus.stopwords.words('english')\n",
    "                    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "                    # Stem the tokens\n",
    "                    stemmer = nltk.stem.PorterStemmer()\n",
    "                    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "                    # Lemmatize the tokens\n",
    "                    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "                    features['number_of_unique_stemmed_words'] = len(set(stemmed_tokens))\n",
    "                    features['number_of_lemmatized_words'] = len(set(lemmatized_tokens))\n",
    "\n",
    "                    cleaned_body = ' '.join([stemmer.stem(token) for token in message_body.split() if token not in stop_words])\n",
    "\n",
    "                    writer.writerow([index, message_body] + list(features.values()) + [cleaned_body] + [target])\n",
    "\n",
    "                    index += 1\n",
    "                    spam_count += 1\n",
    "                    \n",
    "                    if spam_count == 5614:\n",
    "                        break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root_folder = 'data\\enron_eml_spam'\n",
    "    output_csv = 'data\\enron_proccessed_spam.csv'\n",
    "\n",
    "    extract_email_info(root_folder, output_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Assassin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ham 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii: 2409\n",
      "Windows-1252: 45\n",
      "ISO-8859-1: 97\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chardet\n",
    "\n",
    "def get_encoding_counts(directory):\n",
    "  encoding_counts = {}\n",
    "  for file in os.listdir(directory):\n",
    "    encoding = check_encoding(os.path.join(directory, file))\n",
    "    if encoding not in encoding_counts:\n",
    "      encoding_counts[encoding] = 0\n",
    "    encoding_counts[encoding] += 1\n",
    "  return encoding_counts\n",
    "\n",
    "def check_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        encoding = chardet.detect(f.read())['encoding']\n",
    "    return encoding\n",
    "\n",
    "def main():\n",
    "  directory = 'data\\spamassassin\\easy_ham'\n",
    "  encoding_counts = get_encoding_counts(directory)\n",
    "  for encoding, count in encoding_counts.items():\n",
    "    print(f'{encoding}: {count}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def process_file(file_path):\n",
    "  with open(file_path, 'r', encoding='ascii') as file:\n",
    "    content = file.read()\n",
    "  text = content.strip()\n",
    "  return text\n",
    "\n",
    "def main():\n",
    "  directory = 'data\\spamassassin\\easy_ham'\n",
    "  csv_file = open('data\\spamassassin_raw_1.csv', 'w', newline='')\n",
    "  writer = csv.writer(csv_file, delimiter=',')\n",
    "  for file in os.listdir(directory):\n",
    "    encoding = check_encoding(os.path.join(directory, file))\n",
    "    if encoding == 'ascii':\n",
    "      text = process_file(os.path.join(directory, file))\n",
    "      writer.writerow([text])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_features(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', newline='') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        with open(output_csv, 'w', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Number of Unique Stemmed Words', 'Number of Lemmatized Words', 'Cleaned Body', 'Target'])\n",
    "\n",
    "            index = 1\n",
    "            target = 0\n",
    "            for row in reader:\n",
    "                message_body = row[0]\n",
    "\n",
    "                if message_body == '':\n",
    "                    features = {\n",
    "                        'number_of_words': 0,\n",
    "                        'number_of_stop_words': 0,\n",
    "                        'number_of_unique_words': 0,\n",
    "                        'ratio_of_lowercase_to_uppercase': 0,\n",
    "                        'number_of_exclamation_points': 0,\n",
    "                        'number_of_unique_stemmed_words': 0,\n",
    "                        'number_of_lemmatized_words': 0,\n",
    "                        'cleaned_body': '',\n",
    "                    }\n",
    "                else:\n",
    "                    features = {\n",
    "                        'number_of_words': len(message_body.split()),\n",
    "                        'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "                        'number_of_unique_words': len(set(message_body.split())),\n",
    "                        'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "                        'number_of_exclamation_points': message_body.count('!'),\n",
    "                    }\n",
    "\n",
    "                    target = 0 # 0 for ham, 1 for spam\n",
    "\n",
    "                    # Tokenize the message body\n",
    "                    tokens = nltk.word_tokenize(message_body)\n",
    "\n",
    "                    # Remove stop words\n",
    "                    stop_words = nltk.corpus.stopwords.words('english')\n",
    "                    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "                    # Stem the tokens\n",
    "                    stemmer = nltk.stem.PorterStemmer()\n",
    "                    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "                    # Lemmatize the tokens\n",
    "                    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "                    features['number_of_unique_stemmed_words'] = len(set(stemmed_tokens))\n",
    "                    features['number_of_lemmatized_words'] = len(set(lemmatized_tokens))\n",
    "\n",
    "                    cleaned_body = ' '.join([stemmer.stem(token) for token in message_body.split() if token not in stop_words])\n",
    "\n",
    "                    writer.writerow([index, message_body] + list(features.values()) + [cleaned_body] + [target])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_csv = 'data\\spamassassin_raw_1.csv'\n",
    "    output_csv = 'data\\spamassassin_processed_1.csv'\n",
    "\n",
    "    extract_features(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 2410 lines and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def get_dimensions(csv_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        lines = 0\n",
    "        columns = 0\n",
    "        for row in reader:\n",
    "            lines += 1\n",
    "            columns = len(row)\n",
    "    return lines, columns\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    csv_file = 'data/spamassassin_processed_1.csv'\n",
    "    lines, columns = get_dimensions(csv_file)\n",
    "    print(f'The file has {lines} lines and {columns} columns.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ham 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii: 1270\n",
      "None: 120\n",
      "utf-8: 11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chardet\n",
    "\n",
    "def get_encoding_counts(directory):\n",
    "  encoding_counts = {}\n",
    "  for file in os.listdir(directory):\n",
    "    encoding = check_encoding(os.path.join(directory, file))\n",
    "    if encoding not in encoding_counts:\n",
    "      encoding_counts[encoding] = 0\n",
    "    encoding_counts[encoding] += 1\n",
    "  return encoding_counts\n",
    "\n",
    "def check_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        encoding = chardet.detect(f.read())['encoding']\n",
    "    return encoding\n",
    "\n",
    "def main():\n",
    "  directory = 'data\\spamassassin\\easy_ham_2'\n",
    "  encoding_counts = get_encoding_counts(directory)\n",
    "  for encoding, count in encoding_counts.items():\n",
    "    print(f'{encoding}: {count}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401\n"
     ]
    }
   ],
   "source": [
    "print(1270 + 120 + 11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code turns all the ascii files into a .csv with one row, containing the body of the email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "\n",
    "# def process_file(file_path):\n",
    "#   with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#     content = file.read()\n",
    "#   text = content.strip()\n",
    "#   return text\n",
    "\n",
    "# def main():\n",
    "#   directory = 'data\\spamassassin\\easy_ham_2'\n",
    "#   csv_file = open('data\\spamassassin_raw_2.csv', 'w', newline='')\n",
    "#   writer = csv.writer(csv_file, delimiter=',')\n",
    "#   for file in os.listdir(directory):\n",
    "#     text = process_file(os.path.join(directory, file))\n",
    "#     writer.writerow([text])\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def process_file(file_path):\n",
    "  with open(file_path, 'r', encoding='ascii') as file:\n",
    "    content = file.read()\n",
    "  text = content.strip()\n",
    "  return text\n",
    "\n",
    "def main():\n",
    "  directory = 'data\\spamassassin\\easy_ham_2'\n",
    "  csv_file = open('data\\spamassassin_raw_2.csv', 'w', newline='')\n",
    "  writer = csv.writer(csv_file, delimiter=',')\n",
    "  for file in os.listdir(directory):\n",
    "    encoding = check_encoding(os.path.join(directory, file))\n",
    "    if encoding == 'ascii':\n",
    "      text = process_file(os.path.join(directory, file))\n",
    "      writer.writerow([text])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 1270 lines and 1 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def get_dimensions(csv_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        lines = 0\n",
    "        columns = 0\n",
    "        for row in reader:\n",
    "            lines += 1\n",
    "            columns = len(row)\n",
    "    return lines, columns\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    csv_file = 'data/spamassassin_raw_2.csv'\n",
    "    lines, columns = get_dimensions(csv_file)\n",
    "    print(f'The file has {lines} lines and {columns} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_features(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', newline='') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        with open(output_csv, 'w', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Number of Unique Stemmed Words', 'Number of Lemmatized Words', 'Cleaned Body', 'Target'])\n",
    "\n",
    "            index = 1\n",
    "            target = 0\n",
    "            for row in reader:\n",
    "                message_body = row[0]\n",
    "\n",
    "                if message_body == '':\n",
    "                    features = {\n",
    "                        'number_of_words': 0,\n",
    "                        'number_of_stop_words': 0,\n",
    "                        'number_of_unique_words': 0,\n",
    "                        'ratio_of_lowercase_to_uppercase': 0,\n",
    "                        'number_of_exclamation_points': 0,\n",
    "                        'number_of_unique_stemmed_words': 0,\n",
    "                        'number_of_lemmatized_words': 0,\n",
    "                        'cleaned_body': '',\n",
    "                    }\n",
    "                else:\n",
    "                    features = {\n",
    "                        'number_of_words': len(message_body.split()),\n",
    "                        'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "                        'number_of_unique_words': len(set(message_body.split())),\n",
    "                        'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "                        'number_of_exclamation_points': message_body.count('!'),\n",
    "                    }\n",
    "\n",
    "                    target = 0 # 0 for ham, 1 for spam\n",
    "\n",
    "                    # Tokenize the message body\n",
    "                    tokens = nltk.word_tokenize(message_body)\n",
    "\n",
    "                    # Remove stop words\n",
    "                    stop_words = nltk.corpus.stopwords.words('english')\n",
    "                    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "                    # Stem the tokens\n",
    "                    stemmer = nltk.stem.PorterStemmer()\n",
    "                    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "                    # Lemmatize the tokens\n",
    "                    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "                    features['number_of_unique_stemmed_words'] = len(set(stemmed_tokens))\n",
    "                    features['number_of_lemmatized_words'] = len(set(lemmatized_tokens))\n",
    "\n",
    "                    cleaned_body = ' '.join([stemmer.stem(token) for token in message_body.split() if token not in stop_words])\n",
    "\n",
    "                    writer.writerow([index, message_body] + list(features.values()) + [cleaned_body] + [target])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_csv = 'data\\spamassassin_raw_2.csv'\n",
    "    output_csv = 'data\\spamassassin_processed_2.csv'\n",
    "\n",
    "    extract_features(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 1271 lines and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def get_dimensions(csv_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        lines = 0\n",
    "        columns = 0\n",
    "        for row in reader:\n",
    "            lines += 1\n",
    "            columns = len(row)\n",
    "    return lines, columns\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    csv_file = 'data/spamassassin_processed_2.csv'\n",
    "    lines, columns = get_dimensions(csv_file)\n",
    "    print(f'The file has {lines} lines and {columns} columns.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
