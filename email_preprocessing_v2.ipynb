{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import email\n",
    "import re\n",
    "import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from email.message import EmailMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gerba\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Plain Text Files To .eml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eml(root_folder, output_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # Process only files (without extension)\n",
    "            if not os.path.splitext(filename)[1]:\n",
    "                output_path = os.path.join(output_folder, f'{filename}.eml')\n",
    "\n",
    "                try:\n",
    "                    # Read raw text content with UTF-8 encoding\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        raw_message = file.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip files not in UTF-8 encoding\n",
    "                    continue\n",
    "\n",
    "                # Create an EmailMessage object\n",
    "                eml = EmailMessage()\n",
    "                eml.set_content(raw_message)\n",
    "\n",
    "                # Save as EML file\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    output_file.write(eml.as_bytes())\n",
    "\n",
    "# Usage example\n",
    "root_folder = 'data\\enron\\ham'\n",
    "output_folder = 'data\\enron_eml_ham'\n",
    "\n",
    "convert_to_eml(root_folder, output_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eml_spam(root_folder, output_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # Process only files with .txt extension\n",
    "            if os.path.splitext(filename)[1] == '.txt':\n",
    "                output_path = os.path.join(output_folder, f'{filename}.eml')\n",
    "\n",
    "                try:\n",
    "                    # Read raw text content with UTF-8 encoding\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        raw_message = file.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip files not in UTF-8 encoding\n",
    "                    continue\n",
    "\n",
    "                # Create an EmailMessage object\n",
    "                eml = EmailMessage()\n",
    "                eml.set_content(raw_message)\n",
    "\n",
    "                # Save as EML file\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    output_file.write(eml.as_bytes())\n",
    "\n",
    "# Usage example\n",
    "root_folder = 'data\\enron\\spam'\n",
    "output_folder = 'data\\enron_eml_spam'\n",
    "\n",
    "convert_to_eml_spam(root_folder, output_folder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_email_info(root_folder, output_csv):\n",
    "#     with open(output_csv, 'w', newline='') as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "#         writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Target'])\n",
    "\n",
    "#         index = 1\n",
    "\n",
    "#         for root, dirs, files in os.walk(root_folder):\n",
    "#             for filename in files:\n",
    "#                 file_path = os.path.join(root, filename)\n",
    "\n",
    "#                 # Process only files with .eml extension\n",
    "#                 if os.path.splitext(filename)[1] == '.eml':\n",
    "#                     with open(file_path, 'rb') as file:\n",
    "#                         eml_data = file.read()\n",
    "\n",
    "#                     msg = email.message_from_bytes(eml_data)\n",
    "\n",
    "#                     message_body = ''\n",
    "#                     if msg.is_multipart():\n",
    "#                         for part in msg.walk():\n",
    "#                             content_type = part.get_content_type()\n",
    "#                             if content_type == 'text/plain':\n",
    "#                                 message_body = part.get_payload(decode=True).decode('utf-8')\n",
    "#                                 break\n",
    "#                     else:\n",
    "#                         message_body = msg.get_payload(decode=True).decode('utf-8')\n",
    "\n",
    "#                     features = {\n",
    "#                         'number_of_words': len(message_body.split()),\n",
    "#                         'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "#                         'number_of_unique_words': len(set(message_body.split())),\n",
    "#                         'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "#                         'number_of_exclamation_points': message_body.count('!'),\n",
    "#                     }\n",
    "\n",
    "#                     target = 0 # 0 for ham, 1 for spam\n",
    "\n",
    "#                     writer.writerow([index, message_body] + list(features.values()) + [target])\n",
    "\n",
    "#                     index += 1\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     root_folder = 'data\\enron_eml'\n",
    "#     output_csv = 'data\\enron_proccessed.csv'\n",
    "\n",
    "#     extract_email_info(root_folder, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_email_info(root_folder, output_csv):\n",
    "#     with open(output_csv, 'w', newline='') as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "#         writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Number of Unique Stemmed Words', 'Number of Lemmatized Words'])\n",
    "\n",
    "#         index = 1\n",
    "\n",
    "#         for root, dirs, files in os.walk(root_folder):\n",
    "#             for filename in files:\n",
    "#                 file_path = os.path.join(root, filename)\n",
    "\n",
    "#                 # Process only files with .eml extension\n",
    "#                 if os.path.splitext(filename)[1] == '.eml':\n",
    "#                     with open(file_path, 'rb') as file:\n",
    "#                         eml_data = file.read()\n",
    "\n",
    "#                     msg = email.message_from_bytes(eml_data)\n",
    "\n",
    "#                     message_body = ''\n",
    "#                     if msg.is_multipart():\n",
    "#                         for part in msg.walk():\n",
    "#                             content_type = part.get_content_type()\n",
    "#                             if content_type == 'text/plain':\n",
    "#                                 message_body = part.get_payload(decode=True).decode('utf-8')\n",
    "#                                 break\n",
    "#                     else:\n",
    "#                         message_body = msg.get_payload(decode=True).decode('utf-8')\n",
    "\n",
    "#                     features = {\n",
    "#                         'number_of_words': len(message_body.split()),\n",
    "#                         'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "#                         'number_of_unique_words': len(set(message_body.split())),\n",
    "#                         'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "#                         'number_of_exclamation_points': message_body.count('!'),\n",
    "#                     }\n",
    "\n",
    "#                     target = 0 # 0 for ham, 1 for spam\n",
    "\n",
    "#                     # Tokenize the message body\n",
    "#                     tokens = nltk.word_tokenize(message_body)\n",
    "\n",
    "#                     # Remove stop words\n",
    "#                     stop_words = nltk.corpus.stopwords.words('english')\n",
    "#                     tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "#                     # Stem the tokens\n",
    "#                     stemmer = nltk.stem.PorterStemmer()\n",
    "#                     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "#                     # Lemmatize the tokens\n",
    "#                     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#                     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#                     features['number_of_unique_stemmed_words'] = len(set(stemmed_tokens))\n",
    "#                     features['number_of_lemmatized_words'] = len(set(lemmatized_tokens))\n",
    "\n",
    "#                     writer.writerow([index, message_body] + list(features.values()) + [target])\n",
    "\n",
    "#                     index += 1\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     root_folder = 'data\\enron_eml_ham'\n",
    "#     output_csv = 'data\\enron_proccessed.csv'\n",
    "\n",
    "#     extract_email_info(root_folder, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_info(root_folder, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Number of Unique Stemmed Words', 'Number of Lemmatized Words', 'Cleaned Body'])\n",
    "\n",
    "        index = 1\n",
    "\n",
    "        for root, dirs, files in os.walk(root_folder):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "\n",
    "                # Process only files with .eml extension\n",
    "                if os.path.splitext(filename)[1] == '.eml':\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        eml_data = file.read()\n",
    "\n",
    "                    msg = email.message_from_bytes(eml_data)\n",
    "\n",
    "                    message_body = ''\n",
    "                    if msg.is_multipart():\n",
    "                        for part in msg.walk():\n",
    "                            content_type = part.get_content_type()\n",
    "                            if content_type == 'text/plain':\n",
    "                                message_body = part.get_payload(decode=True).decode('utf-8')\n",
    "                                break\n",
    "                    else:\n",
    "                        message_body = msg.get_payload(decode=True).decode('utf-8')\n",
    "\n",
    "                    features = {\n",
    "                        'number_of_words': len(message_body.split()),\n",
    "                        'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "                        'number_of_unique_words': len(set(message_body.split())),\n",
    "                        'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "                        'number_of_exclamation_points': message_body.count('!'),\n",
    "                    }\n",
    "\n",
    "                    target = 0 # 0 for ham, 1 for spam\n",
    "\n",
    "                    # Tokenize the message body\n",
    "                    tokens = nltk.word_tokenize(message_body)\n",
    "\n",
    "                    # Remove stop words\n",
    "                    stop_words = nltk.corpus.stopwords.words('english')\n",
    "                    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "                    # Stem the tokens\n",
    "                    stemmer = nltk.stem.PorterStemmer()\n",
    "                    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "                    # Lemmatize the tokens\n",
    "                    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "                    features['number_of_unique_stemmed_words'] = len(set(stemmed_tokens))\n",
    "                    features['number_of_lemmatized_words'] = len(set(lemmatized_tokens))\n",
    "\n",
    "                    cleaned_body = ' '.join([stemmer.stem(token) for token in message_body.split() if token not in stop_words])\n",
    "\n",
    "                    writer.writerow([index, message_body] + list(features.values()) + [cleaned_body])\n",
    "\n",
    "                    index += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root_folder = 'data\\enron_eml_ham'\n",
    "    output_csv = 'data\\enron_proccessed.csv'\n",
    "\n",
    "    extract_email_info(root_folder, output_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_info(root_folder, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Number of Unique Stemmed Words', 'Number of Lemmatized Words', 'Cleaned Body'])\n",
    "\n",
    "        index = 1\n",
    "\n",
    "        for root, dirs, files in os.walk(root_folder):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "\n",
    "                # Process only files with .eml extension\n",
    "                if os.path.splitext(filename)[1] == '.eml':\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        eml_data = file.read()\n",
    "\n",
    "                    msg = email.message_from_bytes(eml_data)\n",
    "\n",
    "                    message_body = ''\n",
    "                    if msg.is_multipart():\n",
    "                        for part in msg.walk():\n",
    "                            content_type = part.get_content_type()\n",
    "                            if content_type == 'text/plain':\n",
    "                                message_body = part.get_payload(decode=True).decode('utf-8')\n",
    "                                break\n",
    "                    else:\n",
    "                        message_body = msg.get_payload(decode=True).decode('utf-8')\n",
    "\n",
    "                    features = {\n",
    "                        'number_of_words': len(message_body.split()),\n",
    "                        'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "                        'number_of_unique_words': len(set(message_body.split())),\n",
    "                        'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "                        'number_of_exclamation_points': message_body.count('!'),\n",
    "                    }\n",
    "\n",
    "                    target = 1 # 0 for ham, 1 for spam\n",
    "\n",
    "                    # Tokenize the message body\n",
    "                    tokens = nltk.word_tokenize(message_body)\n",
    "\n",
    "                    # Remove stop words\n",
    "                    stop_words = nltk.corpus.stopwords.words('english')\n",
    "                    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "                    # Stem the tokens\n",
    "                    stemmer = nltk.stem.PorterStemmer()\n",
    "                    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "                    # Lemmatize the tokens\n",
    "                    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "                    features['number_of_unique_stemmed_words'] = len(set(stemmed_tokens))\n",
    "                    features['number_of_lemmatized_words'] = len(set(lemmatized_tokens))\n",
    "\n",
    "                    cleaned_body = ' '.join([stemmer.stem(token) for token in message_body.split() if token not in stop_words])\n",
    "\n",
    "                    writer.writerow([index, message_body] + list(features.values()) + [cleaned_body])\n",
    "\n",
    "                    index += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root_folder = 'data\\enron_eml_spam'\n",
    "    output_csv = 'data\\enron_proccessed_spam.csv'\n",
    "\n",
    "    extract_email_info(root_folder, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7f917408d680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0moutput_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data\\enron_proccessed_spam.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mextract_email_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-7f917408d680>\u001b[0m in \u001b[0;36mextract_email_info\u001b[1;34m(root_folder, output_csv)\u001b[0m\n\u001b[0;32m     29\u001b[0m                     features = {\n\u001b[0;32m     30\u001b[0m                         \u001b[1;34m'number_of_words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                         \u001b[1;34m'number_of_stop_words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                         \u001b[1;34m'number_of_unique_words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                         \u001b[1;34m'ratio_of_lowercase_to_uppercase'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7f917408d680>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m                     features = {\n\u001b[0;32m     30\u001b[0m                         \u001b[1;34m'number_of_words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                         \u001b[1;34m'number_of_stop_words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                         \u001b[1;34m'number_of_unique_words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                         \u001b[1;34m'ratio_of_lowercase_to_uppercase'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\learn-env\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def extract_email_info(root_folder, output_csv):\n",
    "#     with open(output_csv, 'w', newline='') as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "#         writer.writerow(['Index', 'Message Body', 'Number of Words', 'Number of Stop Words', 'Number of Unique Words', 'Ratio of Lowercase to Uppercase', 'Number of Exclamation Points', 'Target'])\n",
    "\n",
    "#         index = 1\n",
    "\n",
    "#         for root, dirs, files in os.walk(root_folder):\n",
    "#             for filename in files:\n",
    "#                 file_path = os.path.join(root, filename)\n",
    "\n",
    "#                 # Process only files with .eml extension\n",
    "#                 if os.path.splitext(filename)[1] == '.eml':\n",
    "#                     with open(file_path, 'rb') as file:\n",
    "#                         eml_data = file.read()\n",
    "\n",
    "#                     msg = email.message_from_bytes(eml_data)\n",
    "\n",
    "#                     message_body = ''\n",
    "#                     if msg.is_multipart():\n",
    "#                         for part in msg.walk():\n",
    "#                             content_type = part.get_content_type()\n",
    "#                             if content_type == 'text/plain':\n",
    "#                                 message_body = part.get_payload(decode=True).decode('utf-8')\n",
    "#                                 break\n",
    "#                     else:\n",
    "#                         message_body = msg.get_payload(decode=True).decode('utf-8')\n",
    "\n",
    "#                     features = {\n",
    "#                         'number_of_words': len(message_body.split()),\n",
    "#                         'number_of_stop_words': len([word for word in message_body.split() if word in list(stopwords.words('english'))]),\n",
    "#                         'number_of_unique_words': len(set(message_body.split())),\n",
    "#                         'ratio_of_lowercase_to_uppercase': float(len([word for word in message_body.split() if word.islower()])) / len(message_body.split()),\n",
    "#                         'number_of_exclamation_points': message_body.count('!'),\n",
    "#                     }\n",
    "\n",
    "#                     target = 1 # 0 for ham, 1 for spam\n",
    "\n",
    "#                     writer.writerow([index, message_body] + list(features.values()) + [target])\n",
    "\n",
    "#                     index += 1\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     root_folder = 'data\\enron_eml_spam'\n",
    "#     output_csv = 'data\\enron_proccessed_spam.csv'\n",
    "\n",
    "#     extract_email_info(root_folder, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
